{"paragraphs":[{"text":"%pyspark\n\n#from pyspark import SparkContext, SparkConf #this is correct\nfrom pyspark import * #this is lazy :)\nfrom pyspark.sql import *\nfrom pyspark.sql.types import * #Still have to do this\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import *\nimport datetime","dateUpdated":"2016-12-28T21:10:45+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482959365902_-1934611491","id":"20161228-210925_361028148","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-12-28T21:09:25+0000","dateStarted":"2016-12-28T21:10:45+0000","dateFinished":"2016-12-28T21:10:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3958"},{"text":"%pyspark\n\npath = \"/user/spark/mockCustInfo.csv\"\n\ncsvRDD = sc.textFile(path)\ncsvParsed = csvRDD.map(lambda x: x.split(','))\n\nprint csvParsed.take(5)","dateUpdated":"2016-12-28T21:04:29+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482958686671_1688432104","id":"20161228-205806_587024652","result":{"code":"SUCCESS","type":"TEXT","msg":"[[u'id', u'first_name', u'last_name', u'email', u'gender', u'age', u'salary', u'date_of_birth'], [u'1', u'Joan', u'White', u'jwhite0@phpbb.com', u'Female', u'95', u'173434', u'1974-12-26'], [u'2', u'Kathleen', u'Sullivan', u'ksullivan1@hp.com', u'Female', u'65', u'438805', u'1971-01-19'], [u'3', u'Rebecca', u'Reynolds', u'rreynolds2@xrea.com', u'Female', u'86', u'130123', u'1990-12-04'], [u'4', u'Julia', u'Sullivan', u'jsullivan3@nymag.com', u'Female', u'68', u'24662', u'1993-01-01']]\n"},"dateCreated":"2016-12-28T20:58:06+0000","dateStarted":"2016-12-28T21:04:29+0000","dateFinished":"2016-12-28T21:04:35+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3959"},{"text":"%pyspark\n\n\n\ncolNames = csvParsed.first()\n\n#Exclude the colnames from our data set\n\ncsvParsedBody = csvParsed.filter(lambda line: line != colNames) \n\n#Now we would expect spark sql to be able to deduce the schema...\n\nsqlParsedBody = sqlContext.createDataFrame(csvParsedBody)\n\nfields =  [StructField(field_name, StringType(), True) for field_name in colNames]\n\n#This gives us a tuple of StructField objects, all assigned the type of string\n\n\n#The syntax for the types are absolutely burried in the documentation: https://spark.apache.org/docs/1.6.1/api/python/pyspark.sql.html#pyspark.sql.types.DataType\n\n#But I would rather be a little more confident in what I'm changing as the process was somewhat repetitive\n#I know my attention span, or lake there of, so I was worried about messing something up\n# Which is why I designed this function, also as an exercise to understand the concept better\n\ndef dataChange(fieldTuple, fieldName, dataType):\n\tindex = [y.name for y in fieldTuple].index(fieldName)\n\tfieldTuple[index].dataType = dataType\n\treturn(fieldTuple)\n\nfields = dataChange(fields,'age',IntegerType())\n\nfields = dataChange(fields,'salary',IntegerType())\n\nfields = dataChange(fields,'date_of_birth',DateType())\n\n#DateType was of course giving me a headache\n#Today I remembered why I hate dates in programming!\n#Also why I hate working with csvs in spark to spark sql. It's practically like being a dba...\n\ncsvParsedDateBody = csvParsedBody.map(lambda x: [x[0],x[1],x[2],x[3],x[4],int(x[5]),int(x[6]),datetime.datetime.strptime(x[7],'%Y-%m-%d')])\n","dateUpdated":"2016-12-28T21:11:38+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482959301168_-442798799","id":"20161228-210821_656269205","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-12-28T21:08:21+0000","dateStarted":"2016-12-28T21:11:38+0000","dateFinished":"2016-12-28T21:11:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3960"},{"text":"%pyspark\n\n#finally we can make a schema\n\nschema = StructType(fields)\n\n\n#and finally, finally make a dataframe...\n\nsqlSchemaData = sqlContext.createDataFrame(csvParsedDateBody, schema)\n\n\n#sqlSchemaData.show(5)\n\n\n#+---+----------+---------+--------------------+------+---+------+-------------+\n#| id|first_name|last_name|               email|gender|age|salary|date_of_birth|\n#+---+----------+---------+--------------------+------+---+------+-------------+\n#|  1|      Joan|    White|   jwhite0@phpbb.com|Female| 95|173434|   1974-12-26|\n\n\n#Table for reference\n\n#now for a few basic operations\n\n#Summation\n\ntotalSaldf = sqlSchemaData.select(sum('salary').alias('salary'))\n\nprint \"pyspark aggregation\"\n\nprint totalSaldf.show()\n\n#Summation with sql\n\nsqlSchemaData.registerTempTable(\"sqlTemp\")\n\ntotalSalsql = sqlContext.sql(\"SELECT SUM(salary) AS salary FROM sqlTemp\")\n\nprint \"SQL aggregation\"\n\nprint totalSalsql.show()\n\n#both values are the same and produce new dataframes\n\ntotalSaldfInt = totalSaldf.collect()[0][0]\n\ntotalSalsqlInt = totalSalsql.collect()[0]['salary']\n\nprint (\"Core pyspark.sql gives %s, and sqlContext.sql give %s\" % (totalSaldfInt, totalSalsqlInt))\n\n\n#Aggregation by column values\n\navgGenderSaldf = sqlSchemaData.groupby('gender').avg('salary')\n\nprint \"pyspark.sql aggregation\"\n\nprint avgGenderSaldf.show()\n\navgGenderSalsql = sqlContext.sql(\"SELECT gender,AVG(salary) AS avgSalary FROM sqlTemp GROUP BY gender\")\n\nprint \"sqlContext aggregation\"\n\nprint avgGenderSalsql.show()\n\n\n# Multiple aggregations\n\navgTotGenderSaldf = sqlSchemaData.groupby('gender')\\\n\t.agg(avg('salary').alias('avgSalary'), sum('salary').alias('totSalary'))\n\navgTotGenderSalsql = sqlContext.sql(\"SELECT gender, AVG(salary) AS avgSalary, SUM(salary) AS totSalary FROM sqlTemp GROUP BY gender\")\n\nprint avgTotGenderSaldf.show()\n\nprint avgTotGenderSalsql.show()\n","dateUpdated":"2016-12-28T21:18:59+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","tableHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482959626346_742069638","id":"20161228-211346_1221375285","result":{"code":"SUCCESS","type":"TEXT","msg":"pyspark aggregation\n+---------+\n|   salary|\n+---------+\n|260241229|\n+---------+\n\nNone\nSQL aggregation\n+---------+\n|   salary|\n+---------+\n|260241229|\n+---------+\n\nNone\nCore pyspark.sql gives 260241229, and sqlContext.sql give 260241229\npyspark.sql aggregation\n+------+------------------+\n|gender|       avg(salary)|\n+------+------------------+\n|Female|247919.45808966862|\n|  Male| 273220.8357289528|\n+------+------------------+\n\nNone\nsqlContext aggregation\n+------+------------------+\n|gender|         avgSalary|\n+------+------------------+\n|Female|247919.45808966862|\n|  Male| 273220.8357289528|\n+------+------------------+\n\nNone\n+------+------------------+---------+\n|gender|         avgSalary|totSalary|\n+------+------------------+---------+\n|Female|247919.45808966862|127182682|\n|  Male| 273220.8357289528|133058547|\n+------+------------------+---------+\n\nNone\n+------+------------------+---------+\n|gender|         avgSalary|totSalary|\n+------+------------------+---------+\n|Female|247919.45808966862|127182682|\n|  Male| 273220.8357289528|133058547|\n+------+------------------+---------+\n\nNone\n"},"dateCreated":"2016-12-28T21:13:46+0000","dateStarted":"2016-12-28T21:17:23+0000","dateFinished":"2016-12-28T21:17:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3961"},{"text":"%pyspark\n\n#Practicing window function\n\n#DataFrame API\n\nwindow = Window.partitionBy(\"date_of_birth\").orderBy(\"salary\")\n\nrankSal = sqlSchemaData.select(rank().over(window).alias(\"rank\"), \"first_name\", \"last_name\", \"gender\", \"salary\")\n\nprint rankSal.show(5)","dateUpdated":"2016-12-28T21:24:14+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482958366816_1011013529","id":"20161228-205246_1204441579","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-12-28T20:52:46+0000","dateStarted":"2016-12-28T21:19:22+0000","dateFinished":"2016-12-28T21:19:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3962"},{"text":"%pyspark\n\nrankSalsql = sqlContext.sql(\"SELECT RANK() OVER (PARTITION BY date_of_birth ORDER BY salary) AS salrank, gender, date_of_birth FROM sqlTemp\")\n\nprint rankSalsql.show(5)","dateUpdated":"2016-12-28T21:27:19+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482959599765_-408157718","id":"20161228-211319_960050966","result":{"code":"SUCCESS","type":"TEXT","msg":""},"dateCreated":"2016-12-28T21:13:19+0000","dateStarted":"2016-12-28T21:24:37+0000","dateFinished":"2016-12-28T21:24:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3963"},{"text":"%pyspark\n\n","dateUpdated":"2016-12-28T21:27:19+0000","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482959981984_2109773713","id":"20161228-211941_718200799","result":{"code":"SUCCESS","type":"TEXT","msg":"+-------+------+-------------+\n|salrank|gender|date_of_birth|\n+-------+------+-------------+\n|      1|Female|   1998-01-05|\n|      1|  Male|   1999-08-28|\n|      2|  Male|   1999-08-28|\n|      1|Female|   2005-02-17|\n|      1|  Male|   1958-01-16|\n+-------+------+-------------+\nonly showing top 5 rows\n\nNone\n"},"dateCreated":"2016-12-28T21:19:41+0000","dateStarted":"2016-12-28T21:24:57+0000","dateFinished":"2016-12-28T21:24:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3964"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482960297842_-2071635174","id":"20161228-212457_490778659","dateCreated":"2016-12-28T21:24:57+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:3965"}],"name":"spark","id":"2C76329SQ","angularObjects":{"2BW4S5NJ4:shared_process":[],"2BY1Y2HJG:shared_process":[],"2BVYH21ZJ:shared_process":[],"2BV6637D6:shared_process":[],"2BX5CPDSX:shared_process":[],"2BWHGGFZJ:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}